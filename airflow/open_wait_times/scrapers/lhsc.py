import hashlib
import re

import pandas as pd
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from httpx import get
from pendulum import parse

from .base import BaseScraper


class LHSC(BaseScraper):
    def scrape(self, ts: str) -> dict[str, str]:
        # Initialize hooks
        pg = PostgresHook(postgres_conn_id="owt-pg")
        s3 = S3Hook(aws_conn_id="garage-s3")

        response = get(self.url)

        # Initialize variables
        status_code = response.status_code
        error = None
        file_name = None
        file_hash = None
        skip_downstream = False

        # Capture any HTTP errors
        try:
            response.raise_for_status()
        except Exception as e:
            print(f"Error scraping {self.url}: {e}")
            error = str(e)
            skip_downstream = True

        # Process the response if there are no errors
        if not error:
            file_hash = hashlib.sha256(response.text.encode(encoding="utf-8")).hexdigest()
            file_name = f"{self.name}/{self.dept}/{self._id}_scraper_v{self.version}_{ts}.html"

        # Check if the website has changed since the last fetch
        last_fetch = pg.get_first(
            sql="SELECT file_hash, file_name FROM owt.fetch_logs WHERE hospital_id = %s ORDER BY ts DESC LIMIT 1",
            parameters=[self._id],
        )
        if last_fetch:
            last_fetch_file_hash, last_fetch_file_name = last_fetch
        else:
            last_fetch_file_hash, last_fetch_file_name = None, None

        if file_hash and last_fetch_file_hash == file_hash:
            skip_downstream = True
            file_name = last_fetch_file_name

        # Only upload the file to S3 if it has changed since the last fetch
        if file_hash and last_fetch_file_hash != file_hash:
            s3.load_string(
                string_data=response.text,
                key=file_name,
                bucket_name="open-wait-times",
            )

        # Insert the fetch log into the database
        pg.insert_rows(
            table="owt.fetch_logs",
            rows=[
                (
                    # Fetch log ID will be generated by the database
                    self._id,
                    ts,
                    status_code,
                    error,
                    file_hash,
                    file_name,
                ),
            ],
            target_fields=[
                "hospital_id",
                "ts",
                "status_code",
                "error",
                "file_hash",
                "file_name",
            ],
        )

        fetch_log_id = pg.get_first(
            sql="SELECT id FROM owt.fetch_logs WHERE hospital_id = %s AND ts = %s",
            parameters=[self._id, ts],
        )

        # Let the DAG fail if there is an error
        if error:
            raise Exception(error)

        return {
            "file_name": file_name,
            "fetch_log_id": fetch_log_id,
            "skip_downstream": skip_downstream,
        }

    def parse(self, data: dict[str, str]) -> dict[str, any]:
        s3 = S3Hook(aws_conn_id="garage-s3")
        html = str(s3.read_key(key=data["file_name"], bucket_name="open-wait-times"))

        hospital_identifier = self._id.split("_")[1].upper()
        wait_time_re = (
            f"<!--Start:{hospital_identifier}WaitTimeValue-->(.*?)<!--End:{hospital_identifier}WaitTimeValue-->"
        )
        time_updated_re = (
            f"<!--Start:{hospital_identifier}WaitTimeUpdated-->(.*?)<!--End:{hospital_identifier}WaitTimeUpdated-->"
        )

        wait_time_match = re.search(wait_time_re, html)
        time_updated_match = re.search(time_updated_re, html)

        data["wait_duration"] = wait_time_match.group(1).strip() if wait_time_match else None
        data["update_ts"] = time_updated_match.group(1).strip() if time_updated_match else None

        # Check if the scraped data is newer than the most recently updated data
        pg = PostgresHook(postgres_conn_id="owt-pg")
        update_ts = parse(data["update_ts"], strict=False, tz=self.timezone)

        last_update_ts = pg.get_first(
            sql="SELECT update_ts FROM owt.er_wait_times WHERE hospital_id = %s ORDER BY update_ts DESC LIMIT 1",
            parameters=[self._id],
        )[0]
        last_update_ts = parse(str(last_update_ts), strict=False, tz=self.timezone)

        if last_update_ts >= update_ts:
            print(
                f"Skipping loading data for hospital {self.name} because scraped data ({update_ts}) is older than the most recently updated data ({last_update_ts})"
            )
            data["skip_downstream"] = True

        return data

    def load_data(self, data: dict[str, any]) -> None:
        pg = PostgresHook(postgres_conn_id="owt-pg")

        # We won't do None/Null checks and will rely on pipeline failing to catch this
        hospital_id = self._id
        fetch_log_id = data["fetch_log_id"]
        update_ts = parse(data["update_ts"], strict=False, tz=self.timezone)

        wait_duration = pd.to_timedelta(data["wait_duration"].lower().strip())
        patient_arrival_time = update_ts.subtract(minutes=int(wait_duration.total_seconds() / 60))
        patient_departure_time = update_ts
        extra_info = None

        pg.run(
            """
            INSERT INTO owt.er_wait_times (hospital_id, fetch_log_id, update_ts, wait_duration, patient_arrival_time, patient_departure_time, extra_info)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT DO NOTHING;
            """,
            parameters=(
                hospital_id,
                fetch_log_id,
                update_ts,
                wait_duration,
                patient_arrival_time,
                patient_departure_time,
                extra_info,
            ),
        )
